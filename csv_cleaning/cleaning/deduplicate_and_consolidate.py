import pandas as pd
import os

def deduplicate_and_consolidate():
    # Get the absolute path to the csv_cleaning directory instead of project root
    csv_cleaning_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # Load the consolidated data
    file_path = os.path.join(csv_cleaning_dir, "data", "processed", "standardised_master_consolidated_data.csv")

    # Add debugging prints
    print(f"CSV cleaning directory: {csv_cleaning_dir}")
    print(f"Looking for file at: {file_path}")

    df = pd.read_csv(file_path)

    # Define fields to identify duplicates
    duplicate_criteria = ['vendor_id', 'vendor_name']

    # Function to consolidate duplicate records
    def consolidate_group(group):
        consolidated = group.ffill().bfill().iloc[0]
        
        if 'transaction_total' in group.columns:
            consolidated['transaction_total'] = group['transaction_total'].sum()
        
        return consolidated

    # Group by duplicate criteria and consolidate records
    consolidated_df = df.groupby(duplicate_criteria, as_index=False).apply(consolidate_group)

    # Drop duplicate index generated by grouping
    consolidated_df.reset_index(drop=True, inplace=True)

    # Save the de-duplicated and consolidated file
    output_path = os.path.join(csv_cleaning_dir, "data", "processed", "deduplicated_consolidated_vendor_data.csv")
    consolidated_df.to_csv(output_path, index=False)

    print(f"De-duplicated vendor data saved to: {output_path}")
    
    return consolidated_df

if __name__ == '__main__':
    deduplicate_and_consolidate()