import pandas as pd
import os

# Get the absolute path to the csv_cleaning directory instead of project root
csv_cleaning_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Load the consolidated data
file_path = os.path.join(csv_cleaning_dir, "data", "processed", "standardised_master_consolidated_data.csv")

# Add debugging prints
print(f"CSV cleaning directory: {csv_cleaning_dir}")
print(f"Looking for file at: {file_path}")

df = pd.read_csv(file_path)

# Define fields to identify duplicates
duplicate_criteria = ['vendor_id', 'vendor_name']  # Replace with the fields relevant for your data

# Function to consolidate duplicate records
def consolidate_group(group):
    """
    Consolidate duplicate records by:
    - Forward-filling and back-filling non-null values for completeness.
    - Aggregating numeric columns where appropriate.
    """
    # Merge non-null values for all columns
    consolidated = group.ffill().bfill().iloc[0]
    
    # Example: Aggregate specific columns if needed (adjust as per your data)
    if 'transaction_total' in group.columns:
        consolidated['transaction_total'] = group['transaction_total'].sum()
    
    return consolidated

# Group by duplicate criteria and consolidate records
consolidated_df = df.groupby(duplicate_criteria, as_index=False).apply(consolidate_group)

# Drop duplicate index generated by grouping
consolidated_df.reset_index(drop=True, inplace=True)

# Save the de-duplicated and consolidated file
output_path = os.path.join(csv_cleaning_dir, "data", "processed", "deduplicated_consolidated_vendor_data.csv")
consolidated_df.to_csv(output_path, index=False)

print(f"De-duplicated vendor data saved to: {output_path}")